%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% ICML 2012 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2012,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}

% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure}

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2012} with
% \usepackage[nohyperref]{icml2012} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
%\usepackage{icml2012}
% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Appearing in''
\usepackage[accepted]{icml2012}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Weather Forecasting using Probabilistic Graphical Models}

\begin{document} 

\twocolumn[
\icmltitle{Weather Forecasting using Probabilistic Graphical Models}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2012
% package.
\icmlauthor{Felipe Hern\'andez}{felipeh@andrew.cmu.edu}
\icmladdress{Depertment of Civil and Environmental Engineering, University of Pittsburgh}
\icmlauthor{Amos Ng}{ajng@andrew.cmu.edu}
\icmladdress{Language Technologies Institute, Carnegie Mellon University}

% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Weather forecasting, Gaussian graphical models, land
observations}

\vskip 0.3in
]

\begin{abstract} 

Weather prediction has usually involved running deterministic physical models of
meteorological phenomena in order to predict future conditions or, less
commonly, using non-stochastic machine learning approaches that focus mostly on
precipitation alone on a single location. In this project, we propose to use a
probabilistic machine learning method instead, that takes into account the
spatial distribution of gridded meteorological data products; the interaction
between additional variables like pressure, temperature, radiation, wind speed,
and precipitation; and the inherent uncertainty associated with forecasting. A
Gaussian Graphical Model is used on NASA's NLDAS-2 dataset which offers gridded
multi-variable information on an hourly basis. Although the initial
configurations attempted did not yield very accurate predictions for some
variables, the method adequately captures the forecasting uncertainty and leaves
room for improvement through the use of more expressive input features.

\end{abstract} 

\section{Introduction}
\label{submission}

Weather forecasting is used for a broad range of purposes, ranging from personal
activity planning to large-scale economic decision-making to emergency
preparation and response. The availability and the accuracy of forecasts thus
have a profound impact on human activities at many levels, both in measurable and
unmeasurable aspects.

However, predicting the weather is a difficult research problem. Most often,
physically-based models with global and regional scales are used to forecast
future conditions. In this project, we will instead take a probabilistic machine
learning approach, focused on a regional scale, to predict atmospheric variables
at specific geographic locations. As opposed to deterministic methods, this
probabilistic approach will help us not only to estimate the most probable
values for the forecasting variables as a regression approach would, but also to
determine what the level of uncertainty associated with each forecasted
value is. Uncertainty becomes more important as the forecasting time period
becomes larger. This additional knowledge would be key for potential users in order to
assess how much they can trust the delivered forecasts.

The forecasts are based on prior atmospheric states in the neighborhood of the
selected location. In particular, we attempt to predict the probability
distributions of variables such as pressure, precipitation, and temperature
based on data recorded by NASA using assimilated land products.

\section{Related work} 
 
Researchers in the atmospheric sciences have investigated a variety of methods
for weather forecasting. Many subtle physical phenomena affect the weather at
any given time, including energy and mass fluxes between the sun and different
layers of the atmosphere, ground, and ocean. In order for these phenomena
to be modelled, physically-based equations need to be used to simulate each of
the mass and energy exchanges between the finite volumes that make up the
domain. Machine learning approaches have been explored to construct simplified
models based on atmospheric measurements, but are not very popular among
meteorologists.

Many of these techniques are tailored to fit the nature of the observations
available. Weather monitoring stations are the predominant data source,
providing point measurements with high accuracy and varying temporal resolution.
Artificial neural networks (ANN) have proven to be effective in such cases for
forecasting rainfall amounts in the near future given a time series of
previously observed values, as in the early work of \citet{French1992}. The work
by \citet{maier2000} presents an overview of such works using ANN.

More recent works have attempted to combine ANNs with other techniques to
improve the performance of predictions. In \cite{hong2008}, a recursive ANN is
trained using a support vector regression together with a chaotic particle swarm
optimizer: the particle swarm optimization meta-heuristic is used to find the
optimum parameters of the support vector regression model leading to a better
performance when compared to other ANNs. Similarly, a genetic algorithm is used
to perform the calibration of the network in \cite{nasseri2008}, given that the
defining of the weights between the network's neurons is a non-convex
optimization problem.

A related method is presented by \citet{Partal2007}, where
the results from discrete wavelet transforms over the training data are fed into
a neuro-fuzzy model for prediction. Wavelet neuro-fuzzy approaches are also
studied by \citet{Kisi2011} and contrasted with single neuro-fuzzy models,
single genetic programming regressions, and their conjoint counterparts using
wavelet transformations. Algorithms of different nature than ANNs that are used
in weather forecasting include linear regression, discriminant analysis, and
logistic regression as reviewed in the work by \citet{applequist2002}.

Recently, meteorological observations from land-based and satellite-based
Doppler radars have become widely available, providing enhanced coverage at
the cost of decreased precision. These sources of information add a spatial
dimensions to the forecasting problem. Fourier spectrum, structure function,
and moment-scale analyses are used to understand radar precipitation in
\cite{harris2007}; decision trees are used on a Lagrangian reference framework
to learn rainfall behavior from satellite images in \cite{Yang2007}; and and
several sources of information, including satellite and radar images, are
proposed in \cite{Bartok2010} for the spatial estimation of multiple weather
variables.

Forecasting models have also been created to take advantage
of interdependencies between atmospheric variables that are estimated,
instead of being measured, through the use of other models. For
example, rain-gauge data and outputs from atmospheric models are used for
forecasting precipitation in \cite{kuligowski1998} and \cite{ramirez2005}.
Additional upper air soundings are also used in \cite{Hall1999}, and radar and
satellite data is incorporated into forecasts in \cite{Koizumi1999}.

\section{Data collection}

As mentioned previously, there are multiple sources of meteorological
information available online. Usually these sources are provided by government
agencies such as NOAA and NASA, and have different levels of post-processing of
raw data from gauges, land radars, and satellites.

The North American Land Data Assimilation System (NLDAS), a service hosted by
the Goddard Earth Sciences Data and Information Services Center at NASA, is a
multi-variable source of information produced through the assimilation of land
measurements. We will focus mainly on this dataset due to both its relative high
precision and the multiple variables it reports: precipitation, atmospheric
pressure, humidity, temperature, wind speed, convective potential energy, and
radiation flux.

The NLDAS product provides hourly weather data for the US beginning in 1980.
Each sampled time in the data set includes the following values per cell in a
grid of resolution 1/8 of a degree in both latitude and longitude directions.
Figure~\ref{fig:example_rainfall} provides an illustration of one such hour-long
sample. Table~\ref{tab:variables} lists the full set of variables available at
each spatial cell.

\begin{figure}[ht] \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=1.0\columnwidth]{images/weather.png}}
\caption{This is one example of a rainfall image produced during a severe storm
in Pennsylvania in 2010. Each cell in this image has a size of roughly 10 km
$\times$ 10 km. More information on the NLDAS-2 data products can be found on
the following URLs: \url{http://ldas.gsfc.nasa.gov/index.php} and
\url{http://disc.sci.gsfc.nasa.gov/hydrology/data-holdings}.}
\label{fig:example_rainfall}
\end{center}
\vskip -0.2in
\end{figure}

\begin{table*}[t]
\caption{Meteorological variables available on NLDAS-2.}
\label{tab:variables}
\begin{tabular}{p{4cm} p{8cm} p{3.5cm}}
\hline
\textbf{Name} &\textbf{Description} &\textbf{Units} \\
\hline
precipitation &Accumulated height of precipitated water column &millimeters\\
available potential energy &180-0 mb above ground Convective Potential
Energy &Joules per kilogram \\
\% convective precipitation &Fraction of total precipitation that is convective
&none \\
LW radiation &Long wave radiation flux downwards (surface) &watts per square
meter \\
SW radiation &Short wave radiation flux downwards (surface) &watts per
square meter \\
potential evaporation &Potential evaporation &millimeters \\
surface pressure &Surface pressure &pascals \\
specific humidity &2 m above ground Specific humidity &none \\
temperature 2 m &2 m above ground Temperature &kelvin \\
zonal wind speed &10 m above ground Zonal wind speed &meters per second \\
meridional wind speed &10 m above ground Meridional wind speed &meters per
second \\
\hline
\end{tabular}
\end{table*}

\section{Methods}

The task of forecasting weather states using the NLDAS-2 dataset can be
approached by attempting to estimate the maps for each variable one hour into
the future given the current state maps and, optionally, additional previous
maps. These maps can be obtained by estimating the values for each cell in the
raster one by one. This strategy effectively divides the problem into a
large set of easier sub-problems that can be more easily addressed. By
estimating each value one time step into the future, longer forecasts can be
made by applying the same strategy iteratively on the next time step. This, of
course, would mean an increase in the uncertainty.

Let $X_k^{i,j,t}$ denote variable $k$ at time step $t$ at coordinates $i, j$.
The estimation procedure would rely on a model $f(\cdot)$ that is able to
compute the conditional probability of a variable $X$ given a set of observations
$\hat{X}$ that belong to a spatial frame $S$ and a temporal frame $T$:

\begin{equation}
\label{eq:condProb}
P(X) = f[\{X_k^{i,j,t}\} | (i,j) \in S, t \in T]
\end{equation}

There are several alternatives for determining which variables to use as inputs
of $f(\cdot)$. Some of the alternatives are listed below. Figure
\ref{fig:plate_models} shows the graphical models for each of these cases.

\begin{enumerate}
\item Values on time $t + 1$ depend on the values of all variables of the same
cell on time step $t$
\item Values on time $t + 1$ depend on the values of all variables of the same
cell on time step $t$ and time step $t - 1$ (and maybe time step $t- 2$). We
could use the time derivatives to account for these temporal variations.
\item Values on time $t + 1$ depend on the values of all variables on the
vicinity of the cell at time step $t$. We could account for these spatial
variations by using spatial gradients.
\item Values on time $t + 1$ depend on the values of all variables on the
vicinity of the cell at time step $t$ and time step $t -1$. This option includes
both temporal and spatial derivatives.
\end{enumerate}

\begin{center}
\begin{figure}
\includegraphics[width=1.0\columnwidth]{images/plate_models.png}
\caption{Graphical models to represent different
selections of input variables for the predictive model. \textit{S} represents
a set of neighboring cells and \textit{p} represents the number of previous
time steps to use in addition to time step \textit{t}. 1.
Local dynamic; 2. Local with extended time dependency; 3.
Neighborhood dynamic; 4. Neighborhood with extended time
dependency.}\label{fig:plate_models}
\end{figure}
\end{center}

We chose to model the joint probability distribution of the variables using
Gaussian Graphical Models (GGM) also known as Gaussian Markov Random Fields
(GMRF). These models are multi-variable Gaussian probability distributions with
the particularity that the dependencies between the random variables are
purposefully kept to a minimum. This sparsity allows to create graphical
representations that are meaningful to the users, and to run inference
algorithms with reduced computational requirements. In GGM the density function
of the variable vector $X$ is computed using a mean vector $\mu$ and a
covariance matrix $\Sigma$ using the following expression:

\begin{equation}
\label{eq:gaussian}
P(X|\mu,\Sigma) =
\frac{1}{(2\pi)^{n/2}\vert\Sigma\vert^{1/2}} \exp \left [ -\frac{1}{2}(X -
\mu)^T\Sigma^{-1}(X - \mu) \right ]
\end{equation}

Sparsity in GGMs can be achieved by using regularized linear regressions like
the lasso \cite{tibshirani1996} to estimate the coefficients of the precision
matrices $\Omega=\Sigma^{-1}$. The precision matrix has coefficients with a
value of zero for pairs of variables that are conditionally independent. Two
algorithms that apply this technique for the estimation of the precision matrix
are the neighborhood estimation \cite{meinshausen2006} and the graphical lasso
(glasso - \cite{Friedman2008}). We used glasso for the purposes of this project.

Once we have learned the GGM, we can use inference to perform the forecasting.
Inference implies finding the conditional distributions of all the unknown or
unobserved variables ($X^{\{1\}}$) given the known or observed variables
($X^{\{2\}} = o$). So for example we may want to determine the values of the
variables a time $t + 1$ given the observations at time $t$: $P(X^{t+1} | X^t
= o)$. The conditional distribution is also a multi-variable Gaussian
distribution which can be obtained using a na\"ive matrix inversion approach
described in \cite{Koller2009}: let $\mu_1$ be the vector of mean values of the
unobserved variables, and $\mu_2$ the means of the observed variables.
Similarly, let $\Sigma_{11}$ be the part of the covariance matrix $\Sigma$ with
the covariates between unobserved variables, $\Sigma_{12}$ and $\Sigma_{21}$
with the covariates between unobserved and observed variables, and $\Sigma_{22}$
with the covariates between observed variables. $\mu$ and $\Sigma$ are thus
partitioned as:

\begin{equation}
\label{eq:distPartition}
\mu = \left( \begin{array}{ccc}
\mu_1 \\ \mu_2 
\end{array} \right),
\Sigma = \left( \begin{array}{ccc}
\Sigma_{11} & \Sigma_{12} \\
\Sigma_{21} & \Sigma_{22}
\end{array}\right)
\end{equation}

With this partition we can compute the mean ($\hat\mu$) and the covariance
matrix ($\hat\Sigma$) of the conditional distribution:

\begin{eqnarray}
\label{eq:naiveInference1}
P(X^{\{1\}} | X^{\{2\}} = o) \sim \mathcal{N}(\hat\mu, \hat\Sigma) \\
\label{eq:naiveInference2}
\hat\mu = \mu_1 + \Sigma_{12}\Sigma_{22}^{-1}(o - \mu_2) \\
\label{eq:naiveInference3}
\hat\Sigma = \Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}
\end{eqnarray}

This na\"ive approach requires inverting a matrix, an operation which has a
complexity of $\mathcal{O}(n^3)$. This operation becomes too expensive with
large models and thus more efficient methods become necessary in these cases.
Taking advantage of the graphical structure of the GGM, observed variables can
be eliminated one at a time in an given order so as to minimize the size of the
resulting intermediate terms. This operation marginalizes out each observed
variable until a conditional distribution with only the unobserved variables
remains. In this way, the size of the covariance matrices that are used to
define the intermediate distributions is effectively reduced and so are the
computational requirements.

This is the principle behind the belief propagation algorithm \cite{Koller2009},
which consists in a series of variable eliminations resulting in a set of
messages. These messages are marginalized conditional distributions that are
passed between the nodes of the graph so that the conditional distributions at
each sub-graph can be easily computed. In the case of GGM, variable elimination
consists in performing Gaussian elimination on the probability
distribution of the cliques. The probability distribution of a clique is
a multi-variable Gaussian distribution whose parameters are given by the
elements of the mean vector $\mu$ and the covariance matrix $\Sigma$ that
correspond to the variables in the clique. Variable elimination is performed by
using equations \ref{eq:naiveInference1}, \ref{eq:naiveInference2}, and
\ref{eq:naiveInference3} \cite{malioutov2008approximate}.

However, when the graph contains cycles, it is not possible to perform the
elimination in such a way that all possible dependency paths are considered
simultaneously. Loopy belief propagation \cite{murphy1999} consists in
extending the message passing algorithm to graphical models with cycles.
Although no exact results are obtained, the approximate solutions provided are
good enough in a wide spectrum of cases. Variations of the loopy belief
propagation algorithm exists that look to improve its performance and
scalability. They focus on walking sums that can be used to speed up convergence
at the cost of accuracy, and on the construction of low-rank aliasing matrices
to approximate the inversion of the covariance matrix from the precision matrix
\cite{malioutov2008approximate}.

\section{Results}

We applied our method over an area of roughly 60 km $\times$ 60 km around
Pittsburgh, PA (-80.31° < E < -79.69°, 40.19° < N < 40.81°). This area
has a size of 6 $\times$ 6 pixels in the NLDAS-2 grid. We downloaded one
month of data for this area during a period where heavy rainfall occurred,
between 9/15/2010 and 10/14/2010.

Four test configurations were used: ``static'', which simply correlates
variables in a single cell in the current time step; ``dynamic'', which
estimates the value of each variable given the values in the previous time step;
``spatial gradient'', which uses the same features as the dynamic configuration
but adds the magnitude of the spatial gradient of the values in the previous
time step; and ``temporal gradient'', which adds a temporal gradient, as
measured with the two previous time steps, to the dynamic features.
The addition of the spatial gradient is a compact way to include the values of
adjacent cells into the calculation. Larger neighborhoods of cells can similarly
be taken into account if higher order spatial derivatives are used.
In the same way, higher order temporal derivatives can be used for including
values further back in time.

Table \ref{tab:results_unreg} shows the testing error of the estimated values
for each of the variables and each of the four configurations. The error is the
root mean square error of the estimated posterior means against the testing set
of values as a percentage of the standard deviation of the prior distribution of each
variable. The error was computed using 10-fold cross-validation. We present the
results in this way to standardize the error over variables with different ranges. 

We can see that there is a wide difference in the ability to predict each of the
variables, being the estimation of the potential evaporation the most accurate,
and that of the surface pressure the least accurate. The dynamic approach shows
a big improvement over the static approach, but surprisingly, the addition of
the gradient terms does not improve the accuracy of the prediction
significantly. There is a slight precision improvement for temporal gradients
over spatial gradients.

\begin{table*}[t]
\caption{Root mean square testing error (RMSE) as a fraction of the standard
deviation of the prior of each variable for the four training sets with no
regularization.}
\label{tab:results_unreg}
\centering
\begin{tabular}{p{4cm} p{1.5cm} p{1.5cm} p{1.5cm} p{1.5cm}}
\hline
\textbf{Variable} &\textbf{Static} &\textbf{Dynamic} &\textbf{Spatial
gradient} &\textbf{Temporal gradient} \\
\hline
precipitation 				&0.92 &0.84 &0.86 &0.84 \\
available potential energy	&0.81 &0.21 &0.21 &0.15 \\
\% convective energy		&0.89 &0.72 &0.73 &0.71 \\
LW radiation				&0.61 &0.26 &0.26 &0.25 \\
SW radiation 				&0.42 &0.19 &0.19 &0.16 \\
potential evaporation		&0.32 &0.25 &0.25 &0.22 \\
surface pressure			&0.94 &0.08 &0.08 &0.06 \\
specific humidity			&0.47 &0.16	&0.16 &0.16 \\
temperature					&0.47 &0.09 &0.08 &0.07 \\
wind speed					&0.79 &0.20 &0.20 &0.19 \\
\hline
\end{tabular}
\end{table*}

Table \ref{tab:results_reg} shows the results for the regularized version of the
method. These results are included to determine if the model is overfitting the
data. The regularization constant was chosen with a trial and error approach,
and the best coefficient was chosen for each configuration. The table shows that
the test error increases considerably thanks to regularization, implying that
there is no overfitting occurring, and that the free inclusion of dependencies
between variables is justified.

\begin{table*}[t]
\caption{Root mean square testing error (RMSE) as a fraction of the standard
deviation of the prior of each variable for the four training sets with regularization.}
\label{tab:results_reg}
\centering
\begin{tabular}{p{4cm} p{1.5cm} p{1.5cm} p{1.5cm} p{1.5cm}}
\hline
\textbf{Variable} &\textbf{Static} &\textbf{Dynamic} &\textbf{Spatial
gradient} &\textbf{Temporal gradient} \\
\hline
precipitation 				&0.99 &0.99 &0.98 &0.98 \\
available potential energy	&0.89 &0.22 &0.22 &0.16 \\
\% convective energy		&1.00 &1.00 &1.00 &1.00 \\
LW radiation				&0.88 &0.30 &0.30 &0.30 \\
SW radiation 				&0.91 &0.32 &0.32 &0.18 \\
potential evaporation		&1.00 &1.00 &1.00 &1.00 \\
surface pressure			&0.98 &0.09 &0.09 &0.07 \\
specific humidity			&1.00 &1.00	&1.00 &1.00 \\
temperature					&0.70 &0.63 &0.63 &0.62 \\
wind speed					&0.90 &0.88 &0.88 &0.88 \\
\hline
\end{tabular}
\end{table*}

Figure \ref{fig:uncertainty} shows a scatterplot between the test error and the
estimated posterior standard deviation of the prediction. The plot and the
coefficient of determination $R^2$ of 0.9672 show that the model makes a very
good estimation of the uncertainty of the forecast. The instances of all four
configurations were included in the figure, but taking the unregularized cases
alone would result in an even tighter fit.

\begin{center}
\begin{figure}
\includegraphics[width=1.0\columnwidth]{images/uncertainty.png}
\caption{Scatterplot of the percentage root mean square test error of the
prediction and the ratio between posterior and prior standard deviations. The
close similarity shows that the model adequately estimates the expected error or
uncertainty associated with the prediction through the posterior
variances.}\label{fig:uncertainty}
\end{figure}
\end{center}

\section{Conclusions and future work}

We developed a Gaussian graphical model for the probabilistic forecasting of
meteorological variables. The model was applied on NASA's NLDAS-2 dataset which
offers gridded values for a set of eleven variables every hour. Results show
that some variables are more easily estimated than others, and that the use of a
multi-variable approach can be helpful in the process. Additionally, the
probabilistic nature of the model showed that it can adequately capture the
uncertainty associated with the forecasts. However, the actual accuracy of the
predictions was found to be relatively low for most of the variables.

Future work should focus on exploring more aggressive feature configurations,
given that no overfitting occurred with the features selected in this study. The
possibilities include taking into account higher order spatial and temporal
derivatives of the fields to capture the effect of larger neighboring areas and
longer temporal frames; or directly including the neighbor cell's values within
the training examples. As a complementary approach, higher order polynomial
terms of the features can be used to capture non-linear behavior and
interactions between variables. Finally, the inclusion of variables from
additional atmospheric or land-surface data sets can help in achieving a better
correlated model.

\bibliography{bibliography}
\bibliographystyle{icml2012}

\end{document}

% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was
% created by Lise Getoor and Tobias Scheffer, it was slightly modified  
% from the 2010 version by Thorsten Joachims & Johannes Fuernkranz, 
% slightly modified from the 2009 version by Kiri Wagstaff and 
% Sam Roweis's 2008 version, which is slightly modified from 
% Prasad Tadepalli's 2007 version which is a lightly 
% changed version of the previous year's version by Andrew Moore, 
% which was in turn edited from those of Kristian Kersting and 
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.  



